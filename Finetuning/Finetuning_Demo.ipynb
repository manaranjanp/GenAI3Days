{"cells":[{"cell_type":"markdown","metadata":{"id":"aqlztwUrCUaG"},"source":["# Finetuning using LoRA (Low-Rank Adaptation) for efficient fine-tuning\n","\n","\n","### Developed by Manaranjan Pradhan\n","\n","## Overview\n","\n","This notebook demonstrates how to fine-tune a BLOOM 560M model using LoRA (Low-Rank Adaptation) for efficient fine-tuning.\n","\n","## Dependencies\n","\n","- [bitsandbytes](https://github.com/facebookresearch/bitsandbytes)\n","- [datasets](https://github.com/huggingface/datasets)\n","- [accelerate](https://github.com/huggingface/accelerate)\n","- [loralib](https://github.com/huggingface/peft)\n","- [torch](https://pytorch.org)\n","- [transformers](https://github.com/huggingface/transformers)\n","\n","\n","### Install Dependencies"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3Gy0KqgByeZ","outputId":"c6e64093-1a1b-4e6b-9b78-fa975e5729b6","executionInfo":{"status":"ok","timestamp":1739379673213,"user_tz":-330,"elapsed":31504,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n","Requirement already satisfied: loralib in /usr/local/lib/python3.11/dist-packages (0.1.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Collecting git+https://github.com/huggingface/peft.git\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-t_g95bud\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-t_g95bud\n","  Resolved https://github.com/huggingface/peft.git to commit 363c14e673a12d19f951609d06221962d5c3eb2a\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (2.5.1+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (4.49.0.dev0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (4.67.1)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (1.3.0)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (0.5.2)\n","Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.14.1.dev0) (0.28.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2024.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.14.1.dev0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.14.1.dev0) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.14.1.dev0) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.14.1.dev0) (0.21.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.14.1.dev0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2025.1.31)\n","Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-49zrf3lm\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-49zrf3lm\n","  Resolved https://github.com/huggingface/transformers.git to commit 4a5a7b991a5e9adae78c52bea61fd6e135728622\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.49.0.dev0) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (2024.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.49.0.dev0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.49.0.dev0) (2025.1.31)\n"]}],"source":["# Install several Python packages required for machine learning and natural language processing tasks\n","\n","# Install bitsandbytes: A library for quantization and efficient matrix multiplication\n","# Install datasets: Hugging Face's library for easily accessing and processing datasets\n","# Install accelerate: A library for easy use of distributed training on multiple GPUs/TPUs\n","# Install loralib: A library for Low-Rank Adaptation of Large Language Models\n","# Install torch: PyTorch, a popular deep learning framework\n","!pip install bitsandbytes datasets accelerate loralib torch\n","\n","# Install the latest versions of two Hugging Face libraries directly from their GitHub repositories\n","\n","# Install PEFT (Parameter-Efficient Fine-Tuning) library\n","# This library provides state-of-the-art techniques for efficient fine-tuning of large language models\n","!pip install git+https://github.com/huggingface/peft.git\n","\n","# Install the latest version of the Transformers library\n","# Transformers provides state-of-the-art machine learning for natural language processing tasks\n","!pip install git+https://github.com/huggingface/transformers.git"]},{"cell_type":"markdown","metadata":{"id":"O9pwsBMNCWTA"},"source":["#### Confirm CUDA"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DZAtLKXDB4ko","outputId":"b125258f-8cf3-42f6-f4c8-a8011d825386","executionInfo":{"status":"ok","timestamp":1739379673213,"user_tz":-330,"elapsed":6,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":37}],"source":["import torch\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hu2iqlbAoQqW","outputId":"0d599eef-9ba0-4444-d3ea-6d6e0e8f16a7","executionInfo":{"status":"ok","timestamp":1739379673213,"user_tz":-330,"elapsed":3,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Feb 12 17:01:12 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   73C    P0             32W /   70W |    7200MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"rhn78FE4CZS9"},"source":["#### Load Base Model"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"QoB6pJD0Cejo","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":14643,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["# Import necessary libraries\n","import os\n","import torch\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n","\n","# Uncomment the following line to specify which GPU to use (0 in this case)\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","# Load the pre-trained BLOOM model\n","# AutoModelForCausalLM automatically selects the appropriate model architecture\n","# 'device_map=\"auto\"' allows the model to be automatically distributed across available GPUs\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"bigscience/bloom-560m\",\n","    device_map='auto',\n",")\n","\n","# Load the tokenizer associated with the BLOOM model\n","# The tokenizer is responsible for converting text to tokens that the model can process\n","tokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")"]},{"cell_type":"markdown","metadata":{"id":"Qs2IKgRrCc3v","tags":[]},"source":["##### View Model Summary"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ihHmXefB6i9","outputId":"dcc7c56e-4d4a-49b8-9ebd-cf4d59bb787e","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":10,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["BloomForCausalLM(\n","  (transformer): BloomModel(\n","    (word_embeddings): Embedding(250880, 1024)\n","    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (h): ModuleList(\n","      (0-23): 24 x BloomBlock(\n","        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        (self_attention): BloomAttention(\n","          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n","          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","          (attention_dropout): Dropout(p=0.0, inplace=False)\n","        )\n","        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        (mlp): BloomMLP(\n","          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n","          (gelu_impl): BloomGelu()\n","          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"DFsGQFq-Cejp","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":9,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["# Iterate through all parameters of the model\n","for param in model.parameters():\n","    # Freeze the model parameters\n","    # This prevents the original model weights from being updated during training\n","    param.requires_grad = False\n","\n","    # Check if the parameter is 1-dimensional (e.g., bias terms or layernorm parameters)\n","    if param.ndim == 1:\n","        # Cast 1D parameters to float32 for improved numerical stability\n","        # This is particularly important for small parameters like those in layer normalization\n","        param.data = param.data.to(torch.float32)\n","\n","# Enable gradient checkpointing\n","# This technique reduces memory usage by not storing all activations\n","# Instead, it recomputes them during the backward pass as needed\n","model.gradient_checkpointing_enable()\n","\n","# Enable input gradients\n","# This is necessary for fine-tuning the model with adapters or other techniques\n","model.enable_input_require_grads()\n","\n","# Define a custom layer to cast the output to float32\n","class CastOutputToFloat(nn.Sequential):\n","    def forward(self, x):\n","        # Call the parent class's forward method and cast the output to float32\n","        return super().forward(x).to(torch.float32)\n","\n","# Replace the model's language modeling head with the custom layer\n","# This ensures that the final output is always in float32 precision\n","model.lm_head = CastOutputToFloat(model.lm_head)"]},{"cell_type":"markdown","metadata":{"id":"zdlTV1TNCMm7"},"source":["#### Helper Function"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"Cc8354XxCIWl","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":8,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"markdown","metadata":{"id":"Rc-vqvtuCim3"},"source":["#### Obtain LoRA Model"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQ-cH7ieCARh","outputId":"29894be1-0701-465a-9315-c2d365f1988c","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":8,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 786432 || all params: 560001024 || trainable%: 0.14043402892063284\n"]}],"source":["# Import necessary modules from the PEFT (Parameter-Efficient Fine-Tuning) library\n","from peft import LoraConfig, get_peft_model\n","\n","# Configure LoRA (Low-Rank Adaptation) for efficient fine-tuning\n","config = LoraConfig(\n","    r=8,  # Rank of the update matrices. Lower rank results in fewer trainable parameters\n","    lora_alpha=16,  # Alpha parameter for LoRA scaling. Larger values -> larger updates\n","    target_modules=[\"query_key_value\"],  # Which modules to apply LoRA to. Here, it's applied to attention layers\n","    lora_dropout=0.05,  # Dropout probability for LoRA layers\n","    bias=\"none\",  # Whether to train biases. \"none\" means no bias training\n","    task_type=\"CAUSAL_LM\"  # The type of task. Here, it's causal language modeling\n",")\n","\n","# Apply the LoRA configuration to the model\n","# This wraps the original model with LoRA layers\n","model = get_peft_model(model, config)\n","\n","# Print the number of trainable parameters in the model\n","# This function is not defined in the snippet, but it typically shows the ratio of trainable parameters\n","print_trainable_parameters(model)"]},{"cell_type":"markdown","metadata":{"id":"iopX35giC7L1"},"source":["# Load Dataset and take a sample of the data"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"OoYNglJICejq","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"qzUZvrGLCejq","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["qa_df = pd.read_parquet('train.parquet')"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"d9gQIheCCejq","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"e74d8666-3011-43a5-a25a-0acab0c0b1dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                              id                                title  \\\n","125137  573257950fdd8d15006c69ee  Financial_crisis_of_2007%E2%80%9308   \n","30275   5706b11d0eeca41400aa0d36                          House_music   \n","39176   5ad17e8d645df0001a2d1e38               Mary_(mother_of_Jesus)   \n","32129   5709667eed30961900e840a1                     Himachal_Pradesh   \n","44136   5ad378fa604f3c001a3fe3a1                         Elizabeth_II   \n","\n","                                                  context  \\\n","125137  It threatened the collapse of large financial ...   \n","30275   But house was also being developed on Ibiza,[c...   \n","39176   Although Calvin and Huldrych Zwingli honored M...   \n","32129   Due to extreme variation in elevation, great v...   \n","44136   The Queen addressed the United Nations for a s...   \n","\n","                                                 question  \\\n","125137  What year did the global recession that follow...   \n","30275   what was a popular club in ibiza that started ...   \n","39176   In what century did Martin Luther honor Mary a...   \n","32129                           What is the climate like?   \n","44136         How many times has the Queen toured Canada?   \n","\n","                                                  answers  \n","125137          {'answer_start': [481], 'text': ['2012']}  \n","30275        {'answer_start': [251], 'text': ['Amnesia']}  \n","39176                    {'answer_start': [], 'text': []}  \n","32129   {'answer_start': [115], 'text': ['varies from ...  \n","44136                    {'answer_start': [], 'text': []}  "],"text/html":["\n","  <div id=\"df-de72c340-a9b3-4d26-af48-08a5c7b1700c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>context</th>\n","      <th>question</th>\n","      <th>answers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>125137</th>\n","      <td>573257950fdd8d15006c69ee</td>\n","      <td>Financial_crisis_of_2007%E2%80%9308</td>\n","      <td>It threatened the collapse of large financial ...</td>\n","      <td>What year did the global recession that follow...</td>\n","      <td>{'answer_start': [481], 'text': ['2012']}</td>\n","    </tr>\n","    <tr>\n","      <th>30275</th>\n","      <td>5706b11d0eeca41400aa0d36</td>\n","      <td>House_music</td>\n","      <td>But house was also being developed on Ibiza,[c...</td>\n","      <td>what was a popular club in ibiza that started ...</td>\n","      <td>{'answer_start': [251], 'text': ['Amnesia']}</td>\n","    </tr>\n","    <tr>\n","      <th>39176</th>\n","      <td>5ad17e8d645df0001a2d1e38</td>\n","      <td>Mary_(mother_of_Jesus)</td>\n","      <td>Although Calvin and Huldrych Zwingli honored M...</td>\n","      <td>In what century did Martin Luther honor Mary a...</td>\n","      <td>{'answer_start': [], 'text': []}</td>\n","    </tr>\n","    <tr>\n","      <th>32129</th>\n","      <td>5709667eed30961900e840a1</td>\n","      <td>Himachal_Pradesh</td>\n","      <td>Due to extreme variation in elevation, great v...</td>\n","      <td>What is the climate like?</td>\n","      <td>{'answer_start': [115], 'text': ['varies from ...</td>\n","    </tr>\n","    <tr>\n","      <th>44136</th>\n","      <td>5ad378fa604f3c001a3fe3a1</td>\n","      <td>Elizabeth_II</td>\n","      <td>The Queen addressed the United Nations for a s...</td>\n","      <td>How many times has the Queen toured Canada?</td>\n","      <td>{'answer_start': [], 'text': []}</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de72c340-a9b3-4d26-af48-08a5c7b1700c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-de72c340-a9b3-4d26-af48-08a5c7b1700c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-de72c340-a9b3-4d26-af48-08a5c7b1700c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9b95f194-4867-4112-be84-33c2fde9e7ad\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b95f194-4867-4112-be84-33c2fde9e7ad')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9b95f194-4867-4112-be84-33c2fde9e7ad button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"qa_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"5706b11d0eeca41400aa0d36\",\n          \"5ad378fa604f3c001a3fe3a1\",\n          \"5ad17e8d645df0001a2d1e38\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"House_music\",\n          \"Elizabeth_II\",\n          \"Mary_(mother_of_Jesus)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"But house was also being developed on Ibiza,[citation needed] although no house artists or labels were coming from this tiny island at the time. By the mid-1980s a distinct Balearic mix of house was discernible.[citation needed] Several clubs such as Amnesia with DJ Alfredo were playing a mix of rock, pop, disco and house. These clubs, fueled by their distinctive sound and Ecstasy, began to have an influence on the British scene. By late 1987, DJs such as Trevor Fung, Paul Oakenfold and Danny Rampling were bringing the Ibiza sound to UK clubs such as the Ha\\u00e7ienda in Manchester, and in London clubs such as Shoom in Southwark, Heaven, Future and Spectrum.\",\n          \"The Queen addressed the United Nations for a second time in 2010, again in her capacity as Queen of all Commonwealth realms and Head of the Commonwealth. The UN Secretary General, Ban Ki-moon, introduced her as \\\"an anchor for our age\\\". During her visit to New York, which followed a tour of Canada, she officially opened a memorial garden for the British victims of the September 11 attacks. The Queen's visit to Australia in October 2011 \\u2013 her sixteenth visit since 1954 \\u2013 was called her \\\"farewell tour\\\" in the press because of her age.\",\n          \"Although Calvin and Huldrych Zwingli honored Mary as the Mother of God in the 16th century, they did so less than Martin Luther. Thus the idea of respect and high honor for Mary was not rejected by the first Protestants; but, they came to criticize the Roman Catholics for venerating Mary. Following the Council of Trent in the 16th century, as Marian veneration became associated with Catholics, Protestant interest in Mary decreased. During the Age of the Enlightenment any residual interest in Mary within Protestant churches almost disappeared, although Anglicans and Lutherans continued to honor her.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"what was a popular club in ibiza that started playing dance and house music?\",\n          \"How many times has the Queen toured Canada?\",\n          \"In what century did Martin Luther honor Mary as the Mother of God?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":46}],"source":["qa_df.sample(5)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"Mwra-Bo0Cejq","executionInfo":{"status":"ok","timestamp":1739379687854,"user_tz":-330,"elapsed":5,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from datasets import Dataset"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"eDY2cjjSCejq","executionInfo":{"status":"ok","timestamp":1739379689104,"user_tz":-330,"elapsed":1255,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["train_df = Dataset.from_pandas(qa_df)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"hTI0pl_yN_TO","executionInfo":{"status":"ok","timestamp":1739379689104,"user_tz":-330,"elapsed":8,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["import random\n","\n","num_samples = 1000\n","\n","# Generate random indices\n","random_indices = random.sample(range(len(train_df)), num_samples)\n","\n","# Sample the records\n","sampled_records = train_df.select(random_indices)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mx84SGLN1AB","outputId":"4eb7d2d1-a2ec-4ed0-95a7-cfac48a83fb7","executionInfo":{"status":"ok","timestamp":1739379689104,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Record 1: {'id': '5727ebe03acd2414000deff0', 'title': 'Gamal_Abdel_Nasser', 'context': \"Nasser remains an iconic figure in the Arab world, particularly for his strides towards social justice and Arab unity, modernization policies, and anti-imperialist efforts. His presidency also encouraged and coincided with an Egyptian cultural boom, and launched large industrial projects, including the Aswan Dam and Helwan City. Nasser's detractors criticize his authoritarianism, his government's human rights violations, his populist relationship with the citizenry, and his failure to establish civil institutions, blaming his legacy for future dictatorial governance in Egypt. Historians describe Nasser as a towering political figure of the Middle East in the 20th century.\", 'question': 'What century did Nasser rule in?', 'answers': {'answer_start': [667], 'text': ['20th']}}\n","Record 2: {'id': '56dff532231d4119001abf05', 'title': 'Pub', 'context': 'In Ireland, pubs are known for their atmosphere or \"craic\". In Irish, a pub is referred to as teach tábhairne (\"tavernhouse\") or teach óil (\"drinkinghouse\"). Live music, either sessions of traditional Irish music or varieties of modern popular music, is frequently featured in the pubs of Ireland. Pubs in Northern Ireland are largely identical to their counterparts in the Republic of Ireland except for the lack of spirit grocers. A side effect of \"The Troubles\" was that the lack of a tourist industry meant that a higher proportion of traditional bars have survived the wholesale refitting of Irish pub interiors in the \\'English style\\' in the 1950s and 1960s. New Zealand sports a number of Irish pubs.', 'question': 'What country outside Ireland is known for having Irish pubs?', 'answers': {'answer_start': [664], 'text': ['New Zealand']}}\n","Record 3: {'id': '56d61c4e1c85041400946f1d', 'title': '2008_Sichuan_earthquake', 'context': 'The Internet was extensively used for passing information to aid rescue and recovery efforts. For example, the official news agency Xinhua set up an online rescue request center in order to find the blind spots of disaster recovery. After knowing that rescue helicopters had trouble landing into the epicenter area in Wenchuan, a student proposed a landing spot online and it was chosen as the first touchdown place for the helicopters[not in citation given]. Volunteers also set up several websites to help store contact information for victims and evacuees. On May 31, a rescue helicopter carrying earthquake survivors and crew members crashed in fog and turbulence in Wenchuan county. No-one survived.', 'question': 'What kind of information were websites set up to store?', 'answers': {'answer_start': [514], 'text': ['contact information']}}\n","Record 4: {'id': '572abec0be1ee31400cb8201', 'title': 'Friedrich_Hayek', 'context': 'In Why F A Hayek is a Conservative, British policy analyst Madsen Pirie claims Hayek mistakes the nature of the conservative outlook. Conservatives, he says, are not averse to change – but like Hayek, they are highly averse to change being imposed on the social order by people in authority who think they know how to run things better. They wish to allow the market to function smoothly and give it the freedom to change and develop. It is an outlook, says Pirie, that Hayek and conservatives both share.', 'question': 'Pirie believes Hayek to be a conservative for what reason?', 'answers': {'answer_start': [85], 'text': ['mistakes the nature of the conservative outlook']}}\n","Record 5: {'id': '5a36f18395360f001af1b36c', 'title': 'Gregorian_calendar', 'context': 'To unambiguously specify the date, dual dating or Old Style (O.S.) and New Style (N.S.) are sometimes used with dates. Dual dating uses two consecutive years because of differences in the starting date of the year, or includes both the Julian and Gregorian dates. Old Style and New Style (N.S.) indicate either whether the start of the Julian year has been adjusted to start on 1 January (N.S.) even though documents written at the time use a different start of year (O.S.), or whether a date conforms to the Julian calendar (O.S.) rather than the Gregorian (N.S.).', 'question': 'Which system indicates that the date that the Julian date has been adjusted for length?', 'answers': {'answer_start': [], 'text': []}}\n"]}],"source":[" # Print the first few records from the training set\n","for i in range(5):\n","    print(f\"Record {i+1}: {sampled_records[i]}\")"]},{"cell_type":"markdown","metadata":{"id":"AD0u5hP3WnXf"},"source":["# Creat the fine tuning dataset\n","\n","```\n","### CONTEXT\n","{context}\n","\n","### QUESTION\n","{question}\n","\n","### ANSWER\n","{answer}</s>\n","```"]},{"cell_type":"markdown","metadata":{"id":"fqRzEfHhCoC7"},"source":["# Define a function to create a formatted prompt for question-answering tasks\n","def create_prompt(context, question, answer):\n","    # Check if the answer is empty\n","    if len(answer[\"text\"]) < 1:\n","        # If no answer is found, use a default message\n","        answer = \"Cannot Find Answer\"\n","    else:\n","        # If an answer exists, use the first one (assuming there might be multiple answers)\n","        answer = answer[\"text\"][0]\n","    \n","    # Create a formatted prompt string using f-string\n","    # The prompt includes context, question, and answer, each in its own section\n","    # The '</s>' at the end is likely a special token to indicate the end of the sequence\n","    prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n","    \n","    # Return the formatted prompt\n","    return prompt_template\n","\n","### Apply the create_prompt function to the dataset and tokenize the result\n","\n","This uses the map function to process each sample in the dataset\n","mapped_qa_dataset = sampled_records.map(\n","    lambda samples: tokenizer(\n","        create_prompt(\n","            samples['context'],\n","            samples['question'],\n","            samples['answers']\n","        )\n","    )\n",")#### Train LoRA"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["f762041bf1104218859388bbb16498a9","a1f7f28db8774ce9bafdc914d1ff0c9a","388a2525689d46e9bb296c938af1f26c","f8245e235afc4be2bbf584f98ba47aed","89f2aa9ddacf42f193cb1bd391e2e643","51b5f86aa04d48168af2e5464b774bc1","b10c55835bcd4952aa0d93f21a50908e","7ae1f201ffee460899cb8d59df965888","55b701a0ca9f446a9ba221300b8d8b7e","e6dae54a25c04559ad3a4d3f4ce98d90","7bc72e9f617a4c3c87c8e4a595c6e3ca"]},"id":"3BvDnYjxWPza","outputId":"6adbb0c1-74d3-4a7d-c702-05e83e62e62c","executionInfo":{"status":"ok","timestamp":1739379690868,"user_tz":-330,"elapsed":1770,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f762041bf1104218859388bbb16498a9"}},"metadata":{}}],"source":["# Define a function to create a formatted prompt for question-answering tasks\n","def create_prompt(context, question, answer):\n","    # Check if the answer is empty\n","    if len(answer[\"text\"]) < 1:\n","        # If no answer is found, use a default message\n","        answer = \"Cannot Find Answer\"\n","    else:\n","        # If an answer exists, use the first one (assuming there might be multiple answers)\n","        answer = answer[\"text\"][0]\n","\n","    # Create a formatted prompt string using f-string\n","    # The prompt includes context, question, and answer, each in its own section\n","    # The '</s>' at the end is likely a special token to indicate the end of the sequence\n","    prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n","\n","    # Return the formatted prompt\n","    return prompt_template\n","\n","# Apply the create_prompt function to the dataset and tokenize the result\n","# This uses the map function to process each sample in the dataset\n","mapped_qa_dataset = sampled_records.map(\n","    lambda samples: tokenizer(\n","        create_prompt(\n","            samples['context'],\n","            samples['question'],\n","            samples['answers']\n","        )\n","    )\n",")"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"LNZ3Txn4CFeR","outputId":"d9b51c22-955a-45ba-9c22-5f8f5d7c443a","executionInfo":{"status":"ok","timestamp":1739379828612,"user_tz":-330,"elapsed":8792,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5/5 00:06, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>3.522500</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.457500</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.337900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>3.131600</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>3.187900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=5, training_loss=3.327502155303955, metrics={'train_runtime': 7.9454, 'train_samples_per_second': 10.069, 'train_steps_per_second': 0.629, 'total_flos': 35033499303936.0, 'train_loss': 3.327502155303955, 'epoch': 0.08})"]},"metadata":{},"execution_count":52}],"source":["# Import the transformers library\n","import transformers\n","\n","# Create a Trainer object\n","trainer = transformers.Trainer(\n","    model=model,  # The model to be trained\n","    train_dataset=mapped_qa_dataset,  # The dataset to train on\n","    args=transformers.TrainingArguments(\n","        report_to=\"none\",  # Report training progress to TensorBoard\n","        per_device_train_batch_size=4,  # Number of samples per batch on each device\n","        gradient_accumulation_steps=4,  # Number of steps to accumulate gradients over\n","        max_steps=5,  # Maximum number of training steps\n","        learning_rate=1e-3,  # Learning rate for the optimizer\n","        fp16=True,  # Use 16-bit floating point precision\n","        logging_steps=1,  # Log training metrics every step\n","        output_dir='outputs',  # Directory to save model checkpoints and logs\n","    ),\n","    # Data collator for language modeling tasks\n","    # mlm=False indicates it's not masked language modeling (i.e., it's causal language modeling)\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","\n","# Disable the model's cache to prevent warning messages\n","# Note: This should be re-enabled for inference to improve performance\n","model.config.use_cache = False\n","\n","# Start the training process\n","trainer.train()"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"5_hYow_9fJN8","executionInfo":{"status":"ok","timestamp":1739379831679,"user_tz":-330,"elapsed":905,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["# Save the model to a directory\n","# Save the model to a directory\n","model_save_path = \"./my_finetuned_model\"\n","\n","model.save_pretrained(model_save_path)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pqGsRVVfnUE","outputId":"d6c38ef1-8460-461c-f885-6399a6e5ec63","executionInfo":{"status":"ok","timestamp":1739379837328,"user_tz":-330,"elapsed":5650,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"]}],"source":["# Import necessary libraries\n","import torch\n","from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Load the PEFT configuration from a saved path\n","# This configuration contains information about the fine-tuning setup\n","peft_config = PeftConfig.from_pretrained(model_save_path)\n","\n","# Load the base model specified in the PEFT configuration\n","# AutoModelForCausalLM automatically selects the appropriate model architecture\n","# return_dict=True ensures the model returns outputs as a dictionary\n","# load_in_8bit=False means we're not using 8-bit quantization\n","# device_map='auto' allows the model to be automatically distributed across available GPUs\n","model = AutoModelForCausalLM.from_pretrained(\n","    peft_config.base_model_name_or_path,\n","    return_dict=True,\n","    load_in_8bit=False,\n","    device_map='auto'\n",")\n","\n","# Load the tokenizer associated with the base model\n","# The tokenizer is responsible for converting text to tokens that the model can process\n","tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n","\n","# Apply the PEFT configuration to the loaded model\n","# This step adds the fine-tuned parameters to the base model\n","qa_model = get_peft_model(model, peft_config)"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omsj0qK__Ezs","outputId":"ac086b94-207b-4a9a-9fb7-a653595fb8c3","executionInfo":{"status":"ok","timestamp":1739379837328,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model is on device: cuda:0\n"]}],"source":["# Assuming 'model' is your PyTorch model\n","device = next(qa_model.parameters()).device\n","print(\"Model is on device:\", device)"]},{"cell_type":"markdown","metadata":{"id":"rNrJlaSR9Foy"},"source":["```\n","### CONTEXT\n","{context}\n","\n","### QUESTION\n","{question}\n","\n","### ANSWER\n","{answer}</s>\n","\n","```"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"XAqywI0-ewEK","executionInfo":{"status":"ok","timestamp":1739379837328,"user_tz":-330,"elapsed":5,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["# Import necessary display functions from IPython\n","from IPython.display import display, Markdown\n","\n","# Define a function to perform inference with the question-answering model\n","def make_inference(context, question):\n","    # Create input by formatting context and question\n","    # This follows the format used during training\n","    input_text = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n\"\n","\n","    # Tokenize the input text\n","    # return_tensors='pt' returns PyTorch tensors\n","    batch = tokenizer(input_text, return_tensors='pt')\n","\n","    # Get the device (CPU/GPU) that the model is on\n","    device = next(qa_model.parameters()).device\n","\n","    # Move the input tensors to the same device as the model\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","\n","    # Use CUDA's automatic mixed precision for faster inference (if available)\n","    with torch.cuda.amp.autocast():\n","        # Generate the answer\n","        # max_new_tokens limits the length of the generated answer\n","        output_tokens = qa_model.generate(**batch, max_new_tokens=30)\n","\n","    # Decode the output tokens back to text, skipping special tokens\n","    answer = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n","\n","    # Display the answer as Markdown\n","    # This is useful in Jupyter notebooks for formatted output\n","    display(Markdown(answer))\n","\n","# Note: This function assumes that 'tokenizer' and 'qa_model' are already defined and loaded"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"id":"c2wMsGB8in7F","outputId":"0b781592-e294-4650-ae66-9ab05518cf5e","executionInfo":{"status":"ok","timestamp":1739379848825,"user_tz":-330,"elapsed":11502,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-56-f431f746c3ce>:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"### CONTEXT\n Chandrayaan-3 was launched    Satish Dhawan Space Centre on 14 July 2023. The spacecraft entered lunar orbit on 5 August, \nand the lander touched down near the Lunar south pole on 23 August 2023\n\n### QUESTION\nWhen was chandaryan-3 launched? \n\n### ANSWER\n"},"metadata":{}}],"source":["context = \"\"\" Chandrayaan-3 was launched    Satish Dhawan Space Centre on 14 July 2023. The spacecraft entered lunar orbit on 5 August,\n","and the lander touched down near the Lunar south pole on 23 August 2023\"\"\"\n","\n","question = \"When was chandaryan-3 launched? \"\n","\n","make_inference(context, question)"]},{"cell_type":"code","source":[],"metadata":{"id":"aJaybGfLHfPM"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuClass":"premium","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"default:Python","language":"python","name":"conda-env-default-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f762041bf1104218859388bbb16498a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1f7f28db8774ce9bafdc914d1ff0c9a","IPY_MODEL_388a2525689d46e9bb296c938af1f26c","IPY_MODEL_f8245e235afc4be2bbf584f98ba47aed"],"layout":"IPY_MODEL_89f2aa9ddacf42f193cb1bd391e2e643"}},"a1f7f28db8774ce9bafdc914d1ff0c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51b5f86aa04d48168af2e5464b774bc1","placeholder":"​","style":"IPY_MODEL_b10c55835bcd4952aa0d93f21a50908e","value":"Map: 100%"}},"388a2525689d46e9bb296c938af1f26c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ae1f201ffee460899cb8d59df965888","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55b701a0ca9f446a9ba221300b8d8b7e","value":1000}},"f8245e235afc4be2bbf584f98ba47aed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6dae54a25c04559ad3a4d3f4ce98d90","placeholder":"​","style":"IPY_MODEL_7bc72e9f617a4c3c87c8e4a595c6e3ca","value":" 1000/1000 [00:01&lt;00:00, 978.10 examples/s]"}},"89f2aa9ddacf42f193cb1bd391e2e643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51b5f86aa04d48168af2e5464b774bc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b10c55835bcd4952aa0d93f21a50908e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ae1f201ffee460899cb8d59df965888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55b701a0ca9f446a9ba221300b8d8b7e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6dae54a25c04559ad3a4d3f4ce98d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bc72e9f617a4c3c87c8e4a595c6e3ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}